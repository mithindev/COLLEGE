{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "path2train = \"data/train\"\n",
    "excel_path = os.path.join(path2train, \"image_labels.xlsx\")\n",
    "\n",
    "# Function to create labels DataFrame and save to Excel\n",
    "def create_labels_dataframe(path2train, excel_path):\n",
    "    data = []\n",
    "    index = 0\n",
    "    for img_name in os.listdir(path2train):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            img_path = os.path.join(path2train, img_name)\n",
    "            img = Image.open(img_path)\n",
    "            img = img.resize((224, 224))  # Resize image to (224, 224)\n",
    "            img.save(img_path)  # Save resized image, overwriting the original\n",
    "            \n",
    "            label = 0 if 'cat' in img_name else 1  # Assign label: 0 for cats, 1 for dogs\n",
    "            data.append((index, img_name, label))\n",
    "            index += 1\n",
    "    \n",
    "    labels_df = pd.DataFrame(data, columns=['index', 'imgName', 'label'])\n",
    "    labels_df.to_excel(excel_path, index=False)  # Save to Excel file\n",
    "    return labels_df\n",
    "\n",
    "# Create labels dataframe\n",
    "labels_df = create_labels_dataframe(path2train, excel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Only resizing and normalization for validation\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Update Dataset class to accept different transforms\n",
    "class AMD_dataset(Dataset):\n",
    "    def __init__(self, path2data, transform=None):\n",
    "        self.path2labels = os.path.join(path2data, \"image_labels.xlsx\")\n",
    "        self.labels_df = pd.read_excel(self.path2labels)\n",
    "        self.labels = self.labels_df[\"label\"].values\n",
    "        self.img_names = self.labels_df[\"imgName\"]\n",
    "        self.ids = self.labels_df.index\n",
    "        self.transform = transform\n",
    "        self.path2train = path2data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.path2train, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create dataset instances with the new transforms\n",
    "train_ds = AMD_dataset(path2train, transform_train)\n",
    "val_ds = AMD_dataset(path2train, transform_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "----------\n",
      "112\n",
      "445\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "sss = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "indices = range(len(amd_ds))\n",
    "for train_index, val_index in sss.split(indices):\n",
    "    print(len(train_index))\n",
    "    print(\"-\" * 10)\n",
    "    print(len(val_index))\n",
    "\n",
    "# Create training and validation subsets\n",
    "train_ds = Subset(amd_ds, train_index)\n",
    "val_ds = Subset(amd_ds, val_index)\n",
    "\n",
    "print(len(train_ds))\n",
    "print(len(val_ds))\n",
    "\n",
    "# DataLoader\n",
    "train_dl = DataLoader(train_ds, batch_size=5, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=3, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=12544, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "cpu\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 224, 224]             224\n",
      "       BatchNorm2d-2          [-1, 8, 224, 224]              16\n",
      "            Conv2d-3         [-1, 16, 112, 112]           1,168\n",
      "       BatchNorm2d-4         [-1, 16, 112, 112]              32\n",
      "            Conv2d-5           [-1, 32, 56, 56]           4,640\n",
      "       BatchNorm2d-6           [-1, 32, 56, 56]              64\n",
      "            Conv2d-7           [-1, 64, 28, 28]          18,496\n",
      "       BatchNorm2d-8           [-1, 64, 28, 28]             128\n",
      "            Linear-9                  [-1, 100]       1,254,500\n",
      "           Linear-10                    [-1, 2]             202\n",
      "================================================================\n",
      "Total params: 1,279,470\n",
      "Trainable params: 1,279,470\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.49\n",
      "Params size (MB): 4.88\n",
      "Estimated Total Size (MB): 16.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "# CNN Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "        C_in, H_in, W_in = params[\"input_shape\"]\n",
    "        init_f = params[\"initial_filters\"]\n",
    "        num_fc1 = params[\"num_fc1\"]\n",
    "        num_classes = params[\"num_classes\"]\n",
    "        self.dropout_rate = params[\"dropout_rate\"]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(init_f)\n",
    "        self.conv2 = nn.Conv2d(init_f, 2 * init_f, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * init_f)\n",
    "        self.conv3 = nn.Conv2d(2 * init_f, 4 * init_f, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(4 * init_f)\n",
    "        self.conv4 = nn.Conv2d(4 * init_f, 8 * init_f, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(8 * init_f)\n",
    "\n",
    "        h, w = self.findConv2dOutShape(H_in, W_in, self.conv1)\n",
    "        h, w = self.findConv2dOutShape(h, w, self.conv2)\n",
    "        h, w = self.findConv2dOutShape(h, w, self.conv3)\n",
    "        h, w = self.findConv2dOutShape(h, w, self.conv4)\n",
    "\n",
    "        self.num_flatten = h * w * 8 * init_f\n",
    "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
    "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
    "\n",
    "    def findConv2dOutShape(self, H_in, W_in, conv, pool=2):\n",
    "        kernel_size = conv.kernel_size\n",
    "        stride = conv.stride\n",
    "        padding = conv.padding\n",
    "        dilation = conv.dilation\n",
    "        H_out = np.floor((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = np.floor((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        if pool:\n",
    "            H_out /= pool\n",
    "            W_out /= pool\n",
    "        return int(H_out), int(W_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, self.num_flatten)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, self.dropout_rate, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Model parameters\n",
    "params_model = {\n",
    "    \"input_shape\": (3, 224, 224),\n",
    "    \"initial_filters\": 8,\n",
    "    \"num_fc1\": 100,\n",
    "    \"dropout_rate\": 0.25,\n",
    "    \"num_classes\": 2,\n",
    "}\n",
    "\n",
    "model = Net(params_model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "print(next(model.parameters()).device)\n",
    "\n",
    "summary(model, input_size=(3, 224, 224), device=device.type)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 1.5605\n",
      "Validation Loss: 0.6777, Validation Accuracy: 54.46%\n",
      "Epoch [2/20], Train Loss: 0.6633\n",
      "Validation Loss: 0.6651, Validation Accuracy: 62.50%\n",
      "Epoch [3/20], Train Loss: 0.6430\n",
      "Validation Loss: 0.7011, Validation Accuracy: 58.93%\n",
      "Epoch [4/20], Train Loss: 0.6305\n",
      "Validation Loss: 0.6618, Validation Accuracy: 61.61%\n",
      "Epoch [5/20], Train Loss: 0.5986\n",
      "Validation Loss: 0.6355, Validation Accuracy: 64.29%\n",
      "Epoch [6/20], Train Loss: 0.5500\n",
      "Validation Loss: 0.7438, Validation Accuracy: 58.93%\n",
      "Epoch [7/20], Train Loss: 0.5607\n",
      "Validation Loss: 0.6361, Validation Accuracy: 60.71%\n",
      "Epoch [8/20], Train Loss: 0.5060\n",
      "Validation Loss: 0.5946, Validation Accuracy: 64.29%\n",
      "Epoch [9/20], Train Loss: 0.5062\n",
      "Validation Loss: 0.6443, Validation Accuracy: 65.18%\n",
      "Epoch [10/20], Train Loss: 0.4543\n",
      "Validation Loss: 0.6133, Validation Accuracy: 66.07%\n",
      "Epoch [11/20], Train Loss: 0.4344\n",
      "Validation Loss: 0.6992, Validation Accuracy: 63.39%\n",
      "Epoch [12/20], Train Loss: 0.4523\n",
      "Validation Loss: 0.7420, Validation Accuracy: 68.75%\n",
      "Epoch [13/20], Train Loss: 0.3951\n",
      "Validation Loss: 0.6512, Validation Accuracy: 66.07%\n",
      "Epoch [14/20], Train Loss: 0.3283\n",
      "Validation Loss: 0.8297, Validation Accuracy: 64.29%\n",
      "Epoch [15/20], Train Loss: 0.3100\n",
      "Validation Loss: 0.8243, Validation Accuracy: 66.07%\n",
      "Epoch [16/20], Train Loss: 0.2978\n",
      "Validation Loss: 0.7864, Validation Accuracy: 62.50%\n",
      "Epoch [17/20], Train Loss: 0.2696\n",
      "Validation Loss: 0.8395, Validation Accuracy: 65.18%\n",
      "Epoch [18/20], Train Loss: 0.2565\n",
      "Validation Loss: 0.8407, Validation Accuracy: 63.39%\n",
      "Epoch [19/20], Train Loss: 0.2293\n",
      "Validation Loss: 0.9441, Validation Accuracy: 68.75%\n",
      "Epoch [20/20], Train Loss: 0.2622\n",
      "Validation Loss: 0.8877, Validation Accuracy: 64.29%\n",
      "Training complete. Best Validation Accuracy: 68.75%\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_model(model, train_dl, val_dl, device, num_epochs=20, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_dl:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_dl)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dl:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_running_loss / len(val_dl)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    print(\"Training complete. Best Validation Accuracy: {:.2f}%\".format(best_accuracy))\n",
    "\n",
    "train_model(model, train_dl, val_dl, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/t_rs_h2144q0rmw1kx3__qsc0000gn/T/ipykernel_44954/4017942060.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_190.jpg: cat\n",
      "dog_147.jpg: dog\n",
      "cat_147.jpg: cat\n",
      "dog_219.jpg: dog\n",
      "cat_542.jpg: cat\n",
      "cat_595.jpg: dog\n",
      "dog_191.jpg: dog\n",
      "cat_422.jpg: dog\n",
      "dog_344.jpg: dog\n",
      "dog_150.jpg: dog\n",
      "cat_583.jpg: cat\n",
      "dog_227.jpg: cat\n",
      "dog_421.jpg: cat\n",
      "cat_384.jpg: cat\n",
      "dog_380.jpg: dog\n",
      "dog_155.jpg: dog\n",
      "dog_141.jpg: dog\n",
      "dog_196.jpg: dog\n",
      "dog_551.jpg: cat\n",
      "dog_237.jpg: cat\n",
      "cat_586.jpg: dog\n",
      "cat_545.jpg: dog\n",
      "cat_223.jpg: dog\n",
      "cat_551.jpg: cat\n",
      "cat_587.jpg: dog\n",
      "dog_236.jpg: dog\n",
      "cat_140.jpg: dog\n",
      "dog_197.jpg: cat\n",
      "dog_168.jpg: dog\n",
      "cat_342.jpg: dog\n",
      "dog_28.jpg: cat\n",
      "cat_430.jpg: cat\n",
      "cat_418.jpg: cat\n",
      "cat_395.jpg: cat\n",
      "dog_354.jpg: dog\n",
      "dog_142.jpg: dog\n",
      "dog_181.jpg: dog\n",
      "cat_156.jpg: cat\n",
      "cat_585.jpg: cat\n",
      "cat_234.jpg: cat\n",
      "dog_194.jpg: dog\n",
      "cat_355.jpg: dog\n",
      "cat_433.jpg: cat\n",
      "cat_341.jpg: cat\n",
      "dog_369.jpg: cat\n",
      "dog_355.jpg: dog\n",
      "cat_332.jpg: cat\n",
      "cat_468.jpg: cat\n",
      "dog_124.jpg: dog\n",
      "dog_130.jpg: dog\n",
      "cat_124.jpg: dog\n",
      "cat_118.jpg: dog\n",
      "dog_534.jpg: dog\n",
      "dog_520.jpg: dog\n",
      "cat_520.jpg: cat\n",
      "dog_521.jpg: cat\n",
      "cat_290.jpg: dog\n",
      "cat_119.jpg: dog\n",
      "cat_88.jpg: cat\n",
      "dog_482.jpg: dog\n",
      "dog_59.jpg: dog\n",
      "dog_327.jpg: cat\n",
      "cat_496.jpg: cat\n",
      "dog_443.jpg: dog\n",
      "cat_523.jpg: cat\n",
      "cat_251.jpg: dog\n",
      "cat_279.jpg: dog\n",
      "cat_244.jpg: cat\n",
      "dog_536.jpg: cat\n",
      "dog_244.jpg: dog\n",
      "dog_522.jpg: cat\n",
      "dog_442.jpg: dog\n",
      "cat_60.jpg: dog\n",
      "dog_89.jpg: dog\n",
      "cat_446.jpg: dog\n",
      "dog_240.jpg: dog\n",
      "dog_283.jpg: dog\n",
      "cat_268.jpg: cat\n",
      "cat_255.jpg: dog\n",
      "dog_123.jpg: dog\n",
      "dog_75.jpg: cat\n",
      "cat_109.jpg: cat\n",
      "dog_519.jpg: dog\n",
      "cat_525.jpg: cat\n",
      "dog_518.jpg: dog\n",
      "cat_281.jpg: dog\n",
      "dog_461.jpg: dog\n",
      "dog_313.jpg: cat\n",
      "cat_94.jpg: cat\n",
      "cat_313.jpg: cat\n",
      "cat_1.jpg: dog\n",
      "cat_528.jpg: dog\n",
      "dog_528.jpg: dog\n",
      "dog_44.jpg: dog\n",
      "cat_306.jpg: dog\n",
      "cat_56.jpg: cat\n",
      "dog_476.jpg: cat\n",
      "dog_462.jpg: dog\n",
      "cat_106.jpg: dog\n",
      "dog_258.jpg: cat\n",
      "dog_517.jpg: cat\n",
      "cat_113.jpg: dog\n",
      "cat_96.jpg: cat\n",
      "dog_43.jpg: dog\n",
      "cat_473.jpg: cat\n",
      "cat_116.jpg: cat\n",
      "dog_472.jpg: cat\n",
      "dog_464.jpg: dog\n",
      "dog_302.jpg: dog\n",
      "cat_464.jpg: cat\n",
      "dog_68.jpg: dog\n",
      "dog_114.jpg: dog\n",
      "cat_114.jpg: cat\n",
      "cat_538.jpg: cat\n",
      "cat_504.jpg: dog\n",
      "cat_5.jpg: cat\n",
      "dog_303.jpg: dog\n",
      "dog_364.jpg: dog\n",
      "cat_358.jpg: dog\n",
      "dog_563.jpg: cat\n",
      "dog_211.jpg: cat\n",
      "dog_173.jpg: dog\n",
      "cat_417.jpg: cat\n",
      "cat_371.jpg: dog\n",
      "dog_415.jpg: dog\n",
      "dog_398.jpg: dog\n",
      "dog_159.jpg: dog\n",
      "dog_213.jpg: cat\n",
      "cat_575.jpg: cat\n",
      "cat_574.jpg: dog\n",
      "cat_158.jpg: dog\n",
      "cat_564.jpg: cat\n",
      "cat_203.jpg: dog\n",
      "dog_377.jpg: dog\n",
      "cat_375.jpg: dog\n",
      "dog_177.jpg: dog\n",
      "dog_229.jpg: dog\n",
      "cat_162.jpg: dog\n",
      "cat_18.jpg: cat\n",
      "dog_360.jpg: dog\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Define the model class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "        C_in, H_in, W_in = params[\"input_shape\"]\n",
    "        init_f = params[\"initial_filters\"]\n",
    "        num_fc1 = params[\"num_fc1\"]\n",
    "        num_classes = params[\"num_classes\"]\n",
    "        self.dropout_rate = params[\"dropout_rate\"]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(init_f)\n",
    "        self.conv2 = nn.Conv2d(init_f, 2 * init_f, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * init_f)\n",
    "        self.conv3 = nn.Conv2d(2 * init_f, 4 * init_f, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(4 * init_f)\n",
    "        self.conv4 = nn.Conv2d(4 * init_f, 8 * init_f, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(8 * init_f)\n",
    "\n",
    "        h, w = 224 // 16, 224 // 16  # 224 is input size and we divide by 2^4 because of 4 pooling layers\n",
    "        self.num_flatten = h * w * 8 * init_f\n",
    "\n",
    "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
    "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, self.num_flatten)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, self.dropout_rate, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define the image transformation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define the test dataset class\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, path2data, transform=None):\n",
    "        self.img_names = [f for f in os.listdir(path2data) if f.endswith('.jpg')]\n",
    "        self.path2data = path2data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.path2data, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "path2test = \"data/test\"\n",
    "test_dataset = TestDataset(path2test, transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model parameters\n",
    "params_model = {\n",
    "    \"input_shape\": (3, 224, 224),\n",
    "    \"initial_filters\": 8,\n",
    "    \"num_fc1\": 100,\n",
    "    \"dropout_rate\": 0.25,\n",
    "    \"num_classes\": 2,\n",
    "}\n",
    "\n",
    "# Initialize and load the trained model\n",
    "model = Net(params_model)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Perform inference on the test images\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for images, img_names in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        results.extend(zip(img_names, predicted))\n",
    "\n",
    "# Map predictions to labels\n",
    "label_map = {0: 'cat', 1: 'dog'}\n",
    "predictions = [(img_name, label_map[pred]) for img_name, pred in results]\n",
    "\n",
    "# Print predictions\n",
    "for img_name, label in predictions:\n",
    "    print(f\"{img_name}: {label}\")\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df = pd.DataFrame(predictions, columns=['Image', 'Label'])\n",
    "predictions_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
