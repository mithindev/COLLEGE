{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing BoW from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['great', 'i', 'is', 'love', 'python']\n",
      "Bag of Words Vectors:\n",
      "I love Python: [0, 1, 0, 1, 1]\n",
      "Python is great: [1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)\n",
    "  words = text.split()\n",
    "  return words\n",
    "\n",
    "def bag_of_words(sentences):\n",
    "  vocab = set()\n",
    "  processed = []\n",
    "  \n",
    "  for sentence in sentences:\n",
    "    words = preprocess(sentence)\n",
    "    processed.append(words)\n",
    "    vocab.update(words)\n",
    "\n",
    "  vocab = sorted(vocab)\n",
    "  vocab_dict = {}\n",
    "  \n",
    "  for word in vocab:\n",
    "    vocab_dict[word] = len(vocab_dict)\n",
    "  \n",
    "  bow = []\n",
    "  \n",
    "  for words in processed:\n",
    "    vector = [0] * len(vocab)\n",
    "    for word in words:\n",
    "      vector[vocab_dict[word]] += 1\n",
    "    bow.append(vector)\n",
    "  \n",
    "  return vocab, bow\n",
    "  \n",
    "sentences = [\"I love Python\", \"Python is great\"]\n",
    "vocab, vectors = bag_of_words(sentences)\n",
    "\n",
    "# Display results\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Bag of Words Vectors:\")\n",
    "for sentence, vector in zip(sentences, vectors):\n",
    "    print(f\"{sentence}: {vector}\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['great', 'i', 'is', 'love', 'python']\n",
      "\n",
      "One-Hot Encoding for each word:\n",
      "great: [1, 0, 0, 0, 0]\n",
      "i: [0, 1, 0, 0, 0]\n",
      "is: [0, 0, 1, 0, 0]\n",
      "love: [0, 0, 0, 1, 0]\n",
      "python: [0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = text.split()  # Tokenize (split by spaces)\n",
    "    return words\n",
    "\n",
    "def one_hot_encoding(sentences):\n",
    "    vocab = set()\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = preprocess(sentence)\n",
    "        processed_sentences.append(words)\n",
    "        vocab.update(words)\n",
    "  \n",
    "    vocab = sorted(vocab)  # Sort for consistency\n",
    "    vocab_dict = {}\n",
    "    for word in vocab:\n",
    "        vocab_dict[word] = len(vocab_dict)\n",
    "        \n",
    "    one_hot_vectors = {}\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    for word in vocab:\n",
    "        vector = [0] * vocab_size  # Initialize zero vector\n",
    "        vector[vocab_dict[word]] = 1  # Set the corresponding index to 1\n",
    "        one_hot_vectors[word] = vector\n",
    "\n",
    "    return vocab, one_hot_vectors\n",
    "  \n",
    "sentences = [\"I love Python\", \"Python is great\"]\n",
    "vocab, one_hot_vectors = one_hot_encoding(sentences)\n",
    "\n",
    "# Display results\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"\\nOne-Hot Encoding for each word:\")\n",
    "for word, vector in one_hot_vectors.items():\n",
    "    print(f\"{word}: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF Values: {'python': 0.5596157879354227, 'coding': 0.9162907318741551, 'i': 0.6931471805599453, 'great': 0.9162907318741551, 'is': 0.9162907318741551, 'love': 0.6931471805599453, 'in': 0.9162907318741551}\n",
      "\n",
      "TF-IDF Vectors:\n",
      "I love Python: {'i': 0.23104906018664842, 'love': 0.23104906018664842, 'python': 0.18653859597847422}\n",
      "Python is great: {'python': 0.18653859597847422, 'is': 0.3054302439580517, 'great': 0.3054302439580517}\n",
      "I love coding in Python: {'i': 0.13862943611198905, 'love': 0.13862943611198905, 'coding': 0.18325814637483104, 'in': 0.18325814637483104, 'python': 0.11192315758708454}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Step 2: Define function to preprocess text\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = text.split()  # Tokenize (split by spaces)\n",
    "    return words\n",
    "\n",
    "# Step 3: Define function to compute TF\n",
    "def compute_tf(words):\n",
    "    word_count = Counter(words)  # Count occurrences of each word\n",
    "    total_words = len(words)  # Total number of words in document\n",
    "    tf = {word: count / total_words for word, count in word_count.items()}  # Normalize count\n",
    "    return tf\n",
    "\n",
    "# Step 4: Define function to compute IDF\n",
    "def compute_idf(documents):\n",
    "    total_docs = len(documents)  # Total number of documents\n",
    "    idf_values = {}\n",
    "    all_words = set(word for doc in documents for word in doc)  # Unique words\n",
    "\n",
    "    for word in all_words:\n",
    "        count = sum(1 for doc in documents if word in doc)  # Count docs containing the word\n",
    "        idf_values[word] = math.log((total_docs / (count + 1)) + 1)  # Apply IDF formula\n",
    "\n",
    "    return idf_values\n",
    "\n",
    "# Step 5: Compute TF-IDF\n",
    "def compute_tf_idf(sentences):\n",
    "    processed_sentences = [preprocess(sentence) for sentence in sentences]  # Tokenize each sentence\n",
    "    idf_values = compute_idf(processed_sentences)  # Compute IDF for corpus\n",
    "\n",
    "    tf_idf_vectors = []\n",
    "    for words in processed_sentences:\n",
    "        tf_values = compute_tf(words)  # Compute TF\n",
    "        tf_idf = {word: tf_values[word] * idf_values[word] for word in words}  # TF-IDF formula\n",
    "        tf_idf_vectors.append(tf_idf)\n",
    "\n",
    "    return idf_values, tf_idf_vectors\n",
    "\n",
    "# Example usage\n",
    "sentences = [\"I love Python\", \"Python is great\", \"I love coding in Python\"]\n",
    "idf_values, tf_idf_vectors = compute_tf_idf(sentences)\n",
    "\n",
    "# Display results\n",
    "print(\"IDF Values:\", idf_values)\n",
    "print(\"\\nTF-IDF Vectors:\")\n",
    "for sentence, tf_idf in zip(sentences, tf_idf_vectors):\n",
    "    print(f\"{sentence}: {tf_idf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python program to generate unigrams, bigrams, and trigrams from a given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [['i'], ['love'], ['python'], ['programming']]\n",
      "Bigrams: [['i', 'love'], ['love', 'python'], ['python', 'programming']]\n",
      "Trigrams: [['i', 'love', 'python'], ['love', 'python', 'programming']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = text.split()  # Tokenize (split by spaces)\n",
    "    return words\n",
    "  \n",
    "def generate_ngrams(text, n):\n",
    "  words = preprocess(text)\n",
    "  ngrams = []\n",
    "  for i in range(len(words) - n + 1):\n",
    "    ngrams.append(words[i:i + n])\n",
    "    # ngrams.append(tuple(words[i:i + n]))\n",
    "  return ngrams\n",
    "\n",
    "\n",
    "sentence = \"I love Python programming\"\n",
    "\n",
    "unigrams = generate_ngrams(sentence, 1)\n",
    "bigrams = generate_ngrams(sentence, 2)\n",
    "trigrams = generate_ngrams(sentence, 3)\n",
    "\n",
    "# Display results\n",
    "print(\"Unigrams:\", unigrams)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
